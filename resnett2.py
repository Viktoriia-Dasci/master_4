# -*- coding: utf-8 -*-
"""ResnetT2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GnGCgunOxD7-J7k0ngzxGU13zVOLyvD5
"""

import numpy as np
import nibabel as nib
import glob
import os
import random
import tensorflow as tf
#import splitfolders  # or import split_folders
from tensorflow.keras.utils import to_categorical
import seaborn as sns
from tensorflow.keras.applications import EfficientNetB0
import matplotlib.pyplot as plt
from tifffile import imsave
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.models import Sequential
from sklearn.metrics import classification_report,confusion_matrix
from PIL import Image
from keras.models import load_model
from skimage.color import rgb2gray
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
from torchvision.models import ResNet50_Weights

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, TensorDataset
from torch.nn import functional as F


# Define the device to be used for training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

HGG_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/TrainT2/HGG_t2/*.nii'))
LGG_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/TrainT2/LGG_t2/*.nii'))

#train HGG
HGG_train_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/train/HGG_t2/*.nii'))
#HGG_mask_train_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/train/HGG_masks/*.nii'))
#val HGG
HGG_val_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/val/HGG_t2/*.nii'))
#HGG_mask_val_list_t2 = sorted(glob.glob('/content/drive/MyDrive/T2_split/val/HGG_masks/*.nii'))
#test HGG
HGG_test_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/test/HGG_t2/*.nii'))
#HGG_mask_test_list_t2 = sorted(glob.glob('/content/drive/MyDrive/T2_split/test/HGG_masks/*.nii'))

#train LGG
LGG_train_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/train/LGG_t2/*.nii'))
#LGG_mask_train_list_t2 = sorted(glob.glob('/content/drive/MyDrive/T2_split/train/LGG_masks/*.nii'))
#val LGG
LGG_val_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/val/LGG_t2/*.nii'))
#LGG_mask_val_list_t2 = sorted(glob.glob('/content/drive/MyDrive/T2_split/val/LGG_masks/*.nii'))
#test LGG
LGG_test_list_t2 = sorted(glob.glob('/home/viktoriia.trokhova/T2_split/test/LGG_t2/*.nii'))
#LGG_mask_test_list = sorted(glob.glob('/content/drive/MyDrive/T2_split/test/LGG_masks/*.nii'))

HGG_train = []
LGG_train = []
HGG_val = []
LGG_val = []
HGG_test = []
LGG_test = []
for img in range(len(HGG_train_list_t2)):   #Using t1_list as all lists are of same size   
    train_image_t2_HGG = nib.load(HGG_train_list_t2[img]).get_fdata()
    HGG_train.append(train_image_t2_HGG)

for img in range(len(LGG_train_list_t2)):
    train_image_t2_LGG = nib.load(LGG_train_list_t2[img]).get_fdata()
    LGG_train.append(train_image_t2_LGG)

for img in range(len(HGG_val_list_t2)):   #Using t1_list as all lists are of same size   
    val_image_t2_HGG = nib.load(HGG_val_list_t2[img]).get_fdata()
    HGG_val.append(val_image_t2_HGG)

for img in range(len(LGG_val_list_t2)):
    val_image_t2_LGG = nib.load(LGG_val_list_t2[img]).get_fdata()
    LGG_val.append(val_image_t2_LGG)

for img in range(len(HGG_list_t2)):   #Using t1_list as all lists are of same size   
    test_image_t2_HGG = nib.load(HGG_test_list_t2[img]).get_fdata()
    HGG_test.append(test_image_t2_HGG)

for img in range(len(LGG_list_t2)):
    test_image_t2_LGG = nib.load(LGG_test_list_t2[img]).get_fdata()
    HGG_test.append(test_image_t2_LGG)

# Put X and y to device
#X = torch.tensor(X, dtype=torch.float32).to(device)
#y = torch.tensor(y, dtype=torch.long).to(device)

# Combine the HGG and LGG lists
X_train = np.array(HGG_train + LGG_train)
y_train = np.array([0] * len(HGG_train) + [1] * len(LGG_train))

X_val = np.array(HGG_val + LGG_val)
y_val = np.array([0] * len(HGG_val) + [1] * len(LGG_val))

X_test = np.array(HGG_test + LGG_test)
y_test = np.array([0] * len(HGG_test) + [1] * len(LGG_test))


# Print the shapes of the train and test sets
print('X_train shape:', X_train.shape)
print('y_train shape:', y_train.shape)
print('X_train shape:', X_val.shape)
print('y_train shape:', y_val.shape)
print('X_test shape:', X_test.shape)
print('y_test shape:', y_test.shape)

class SelfAttention(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, channels, height, width = x.size()
        proj_query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)
        proj_key = self.key(x).view(batch_size, -1, height * width)
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        proj_value = self.value(x).view(batch_size, -1, height * width)
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, channels, height, width)
        out = self.gamma * out + x
        return out


class MyCustomResnet50(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        
        resnet50 = models.resnet50(pretrained=True)
        # Replace the first convolutional layer to handle images with shape (240, 240, 155)
        resnet50.conv1 = nn.Conv2d(155, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.features = nn.ModuleList(resnet50.children())[:-2]
        self.features = nn.Sequential(*self.features)
        in_features = resnet50.fc.in_features
        self.attention = SelfAttention(2048)
        self.last_pooling_operation = nn.AdaptiveAvgPool2d((1, 1))
        self.fc1 = nn.Linear(2048, 128)
        self.fc2 = nn.Linear(128, 2)

    def forward(self, input_imgs, batch_size = None, xe_criterion=nn.CrossEntropyLoss(), l1_criterion=nn.L1Loss(), dropout=None):
        images_feats = self.features(input_imgs)
        images_att = self.attention(images_feats)
        output = self.last_pooling_operation(images_att)
        output = output.view(input_imgs.size(0), -1)
        images_outputs = self.fc1(output)
        output = dropout(images_outputs)
        images_outputs = F.relu(self.fc2(output))
        #images_outputs = nn.ReLU(self.fc2(output))


#         # # compute gcam for images
#         orig_gradcam_mask = compute_gradcam(images_outputs, images_feats, targets)

#         # #upsample gradcam to (224, 224, 3)
#         gcam_losses = 0.0

#         for i in range(batch_size):
#             #print(orig_gradcam_mask[i].shape)
#             img_grad = orig_gradcam_mask[i].unsqueeze(0).permute(1, 2, 0)
#             img_grad_1 = img_grad.cpu()
#             img_grad_2 = img_grad_1.detach().numpy()
#             img_grad_3 = cv2.resize(img_grad_2, (224,224), cv2.INTER_LINEAR)
#             img_grad_4 = cv2.cvtColor(img_grad_3, cv2.COLOR_GRAY2RGB)
#             img_grad_5 = torch.from_numpy(img_grad_4)
#             img_grad_6 = img_grad_5.to(device)
#             #img_grad_6 = torch.nn.ReLU(inplace=True)(img_grad_6)


#             #masks to same dimension
#             masks_per = masks[i].permute(1, 2, 0)
#             masks_per = cv2.normalize(masks_per.cpu().numpy(), None, alpha = 0, beta = 1, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)
#             img_grad_6 = cv2.normalize(img_grad_6.cpu().numpy(), None, alpha = 0, beta = 1, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)
#             masks_per[np.mean(masks_per, axis=-1)<0.2] = 0
#             masks_per[np.mean(masks_per, axis=-1)>=0.2] = 1

#             gcam_loss = foc_loss(torch.from_numpy(img_grad_6), torch.from_numpy(masks_per))
#             #print(gcam_loss)
#             #gcam_loss = l1_criterion(img_grad_6, masks_per)
#             gcam_losses += gcam_loss

#             # gcam_loss = l1_criterion(img_grad_6, masks_per)
#             # gcam_losses += gcam_loss

#             #gcam_losses += gcam_loss.item() * input_imgs.size(0)
#         #gcam_losses = gcam_losses/batch_size
#         xe_loss = xe_criterion(images_outputs, targets)
        

        return images_outputs  #return images_outputs



# Define the transformation to be applied to the images
transform = transforms.Compose([
    transforms.Resize((240, 240)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Convert the train, val and test data to PyTorch tensors
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).long()
X_val = torch.from_numpy(X_val).float()
y_val = torch.from_numpy(y_val).long()
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).long()

# Define the test dataset
test_dataset = TensorDataset(X_test, y_test)

# Define the dataset
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
test_dataset = TensorDataset(X_test, y_test)

# Define the data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model
model = MyCustomResnet50().to(device)

EPOCHS=30


def train_and_evaluate(model):
    accuracies = []
    #dataloaders = load_data(batch_size=32)
    # Freeze all layers

    #criterion = nn.CrossEntropyLoss()


    optimizer = optim.SGD(model.parameters(), lr=0.004)
    #optimizer = getattr(optim, "SGD")(model.parameters(), lr=0.004)

    for epoch_num in range(EPOCHS):
            torch.cuda.empty_cache()
            model.train()
            total_acc_train = 0
            total_loss_train = 0

            for train_input, train_label, train_mask in train_loader:

                train_label = train_label.long().to(device)
                train_input = train_input.float().to(device)
                train_mask = train_mask.to(device)

                output = model(train_input, batch_size = train_input.size(0), dropout=nn.Dropout(0.79))
                
                batch_loss = xe_loss_.mean() + 0.575 * gcam_losses_
                #batch_loss = xe_loss_.mean()
                total_loss_train += batch_loss.item()
                
                acc = (output.argmax(dim=1) == train_label).sum().item()
                total_acc_train += acc

                model.zero_grad()
                batch_loss.backward()
                optimizer.step()
            
            total_acc_val = 0
            total_loss_val = 0

            model.eval()
            # with torch.no_grad():
            

            for val_input, val_label, val_mask in val_loader:

                val_label = val_label.long().to(device)
                val_input = val_input.float().to(device)
                val_mask = val_mask.to(device)
                

                output = model(val_input, batch_size = val_input.size(0), dropout=nn.Dropout(0.79))

                batch_loss = xe_loss_.mean() + 0.575 * gcam_losses_
                #batch_loss = xe_loss_.mean()
                total_loss_val += batch_loss.item()
                
                acc = (output.argmax(dim=1) == val_label).sum().item()
                total_acc_val += acc
        
            accuracy = total_acc_val/len(image_datasets['Val'])
            accuracies.append(accuracy)
            print(accuracy)
            if len(accuracies) >= 3 and accuracy <= 0.5729:
                break

#             trial.report(accuracy, epoch_num)
#             if trial.should_prune():
#                 raise optuna.exceptions.TrialPruned()
    final_accuracy = max(accuracies)
    PATH = '/home/viktoriia.trokhova/model_weights/model_best.pt'
    torch.save(model.state_dict(), PATH)
  
    return final_accuracy
  
  
train_and_evaluate(model)
